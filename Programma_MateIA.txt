Il Nursery Database è stato derivato da un modello decisionale gerarchico originariamente sviluppato per classificare le domande di ammissione agli asili. È stato utilizzato per diversi anni negli anni ’80, quando a Lubiana, in Slovenia, vi era un’eccessiva richiesta di iscrizione a questi asili, e spesso le domande rifiutate richiedevano una spiegazione oggettiva.

La decisione finale dipendeva da tre sottoproblemi:
	•	occupazione dei genitori e presenza dell’asilo per il bambino,
	•	struttura familiare e situazione finanziaria,
	•	quadro sociale e sanitario della famiglia.

Il modello è stato sviluppato all’interno del sistema esperto per il supporto decisionale DEX
(M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.)

Il modello gerarchico classifica le domande di ammissione agli asili secondo la seguente struttura concettuale:

 NURSERY            Evaluation of applications for nursery schools
 . EMPLOY           Employment of parents and child's nursery
   . parents        Parents' occupation
   . has_nurs       Child's nursery
 . STRUCT_FINAN     Family structure and financial standings
   . STRUCTURE      Family structure
     . form         Form of the family
     . children     Number of children
   . housing        Housing conditions
   . finance        Financial standing of the family
 . SOC_HEALTH       Social and health picture of the family
   . social         Social conditions
   . health         Health conditions

Gli attributi in input sono scritti in minuscolo. Oltre al concetto obiettivo (NURSERY), il modello include quattro concetti intermedi:
EMPLOY, STRUCT_FINAN, STRUCTURE, SOC_HEALTH.
Ogni concetto nel modello originale è collegato ai propri discendenti di livello inferiore attraverso un insieme di esempi (per questi set di esempi, vedi: http://www-ai.ijs.si/BlazZupan/nursery.html).

Il Nursery Database contiene esempi con le informazioni strutturali rimosse, ossia collega direttamente il concetto NURSERY agli otto attributi in input: parents,has_nurs,form,children,housing,finance,social,health.

Grazie alla struttura concettuale sottostante nota, questo database può essere particolarmente utile per testare metodi di induzione costruttiva e scoperta di strutture.

A lezione ci hanno insegnato questi argomenti:
- One-hot Encoding
- PCA
- Standard scaler
- Scree plot
- Score Graph
- Loading Graph
- Eigenfaces
- LDA
- FDA
- QDA
- SVM lineare
- SVM non lineare
- Matrice di confusione
- Precision
- Recall
- F-beta score
- Training, Validation e Test Set
- Grid Search
- MLP

⸻⸻

Preprocessing

	•	One-hot Encoding: Trasforma variabili categoriche in variabili numeriche binarie (0 o 1), creando una colonna per ogni categoria. Utile perché molti modelli non accettano dati testuali.
	•	Standard Scaler: Porta ogni feature numerica ad avere media 0 e deviazione standard 1. Serve soprattutto prima di tecniche che usano distanze (PCA, SVM, MLP).

⸻⸻

Riduzione Dimensionale e Analisi

	•	PCA (Principal Component Analysis): Trasforma le feature originali in nuove feature (componenti principali), ordinate per varianza. Serve a ridurre la dimensionalità.
	•	Scree Plot: Grafico che mostra quanta varianza spiega ogni componente principale. Aiuta a scegliere quanti componenti tenere.
	•	Score Graph: Mostra i dati proiettati nelle nuove componenti (es. i primi 2 PC) per visualizzare la separazione tra classi.
	•	Loading Graph: Mostra quanto ogni feature originale contribuisce ai componenti principali.
	•	Eigenfaces: Metodo PCA applicato alle immagini di volti (non serve in questo dataset).
	•	LDA (Linear Discriminant Analysis) e FDA (Fisher Discriminant Analysis): Tecniche di proiezione supervisionata, cercano direzioni che massimizzano la separazione tra classi.

⸻⸻

Classificatori

	•	SVM lineare / non lineare: Separano le classi cercando un iperpiano (lineare o con kernel) che massimizza il margine tra i punti di classi diverse.
	•	QDA (Quadratic Discriminant Analysis): Classificatore basato su gaussiane con covarianze diverse per ogni classe.
	•	MLP (Multi-Layer Perceptron): Rete neurale feed-forward usata per classificazione. Richiede scaling e tuning dei parametri.

⸻⸻

Valutazione Modelli

	•	Confusion Matrix: Matrice che mostra TP, FP, TN, FN.
	•	Precision: TP / (TP + FP) — quanto è preciso il modello quando predice una classe.
	•	Recall: TP / (TP + FN) — quanto bene il modello trova tutti i veri positivi.
	•	F-beta Score: Combina precision e recall in un solo valore (F1 score è F-beta con β=1).
	•	Training / Validation / Test Set: Suddivisione del dataset per addestramento, scelta dei parametri e valutazione finale.
	•	Grid Search: Ricerca esaustiva dei migliori iperparametri di un modello.

⸻⸻

Ecco un piano di studio completo:

	1	Caricamento e preprocessing:

		•	Leggi il dataset.
		•	Applica One-hot Encoding a tutte le feature categoriche.
		•	Applica uno StandardScaler per normalizzare i dati.

	2	Visualizzazione e analisi esplorativa:

		•	Calcola la PCA.
		•	Visualizza uno Scree Plot.
		•	Fai uno Score Graph per vedere se le classi si separano già nei primi due componenti.
		•	Analizza i Loading Graph per capire quali feature influenzano di più i componenti.

	3	Riduzione supervisionata:

		•	Applica LDA/FDA per vedere se le classi si separano meglio rispetto a PCA.

	4	Classificazione:

		•	Scegli 2-3 modelli (es. SVM lineare, SVM con RBF, QDA, MLP).
		•	Fai GridSearchCV con cross-validation per scegliere i migliori iperparametri.
		•	Dividi il dataset in training/validation/test.

	5	Valutazione:

		•	Calcola Confusion Matrix, Precision, Recall, F1 score sul test set.
		•	Confronta le prestazioni dei modelli.
		•	Fai grafici comparativi delle metriche.

⸻⸻

Obiettivo

Lo scopo dell’analisi è costruire un modello che predica in modo accurato la decisione finale (NURSERY) basandosi su 8 variabili categoriali. Vogliamo anche capire quali feature sono più influenti e verificare se esistono ridondanze informative tra le variabili.

⸻⸻

1 Preprocessing
  •	One-hot Encoding:
Ho trasformato le variabili categoriali in binarie per poterle usare nei modelli numerici (es. PCA, SVM, MLP), che non possono operare direttamente su stringhe.
  •	Standard Scaler:
Ho normalizzato le feature per evitare che alcune (con più categorie) dominassero l’analisi rispetto ad altre. Questo è importante per PCA, LDA e modelli come SVM o MLP, che sono sensibili alla scala.

⸻⸻

2 Analisi Esplorativa e Riduzione Dimensionale
  •	PCA:
Ho applicato PCA per ridurre la dimensionalità e capire se i dati possono essere rappresentati in uno spazio più semplice mantenendo l’informazione. Questo aiuta a visualizzare meglio i dati e riduce il rumore.
  •	Scree Plot:
Per decidere quanti componenti tenere, ho usato uno scree plot per vedere dove c’è un “ginocchio” (elbow), cioè il punto oltre il quale i componenti aggiuntivi spiegano pochissima varianza.
  •	Score Graph + Loading Graph:
Ho visualizzato i dati nei primi due componenti (score graph) per vedere se c’è separazione tra le classi. Il loading graph mi ha permesso di capire quali feature influenzano di più i componenti.
  •	LDA/FDA:
Ho poi usato LDA perché, a differenza di PCA, considera le etichette e cerca una proiezione che massimizzi la separazione tra le classi. È utile per visualizzare la discriminabilità tra le decisioni finali.

⸻⸻

3 Modellazione
  •	Scelta dei modelli:
Ho provato più classificatori (SVM, QDA, MLP) per confrontare approcci lineari, non lineari e neurali. Questo permette di vedere quale modello è più adatto al tipo di dati.
  •	Grid Search + Validation Set:
Ho usato Grid Search con cross-validation per scegliere i migliori iperparametri, evitando l’overfitting. Ho tenuto un test set separato per valutare in modo onesto il modello finale.

⸻⸻

4 Valutazione
  •	Matrice di Confusione e Metriche:
Ho analizzato precision, recall e F1 score per ciascuna classe, perché il dataset ha 5 classi sbilanciate (not_recom, recommend, very_recom, priority, spec_prior). La sola accuratezza non basta per capire se il modello sta performando bene su tutte le classi.

⸻⸻

Conclusione

Questo approccio mi ha permesso non solo di classificare bene le domande d’ammissione, ma anche di capire come le variabili influenzano la decisione, e quali tecniche di riduzione o classificazione funzionano meglio per questo tipo di dati categoriali strutturati.